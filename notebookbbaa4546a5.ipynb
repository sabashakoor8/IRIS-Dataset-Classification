{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T15:12:00.775621Z","iopub.execute_input":"2024-08-17T15:12:00.776128Z","iopub.status.idle":"2024-08-17T15:12:05.893342Z","shell.execute_reply.started":"2024-08-17T15:12:00.776092Z","shell.execute_reply":"2024-08-17T15:12:05.891646Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the Iris dataset\niris = load_iris()\nX = iris.data  # Features\ny = iris.target  # Labels\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T15:35:12.977321Z","iopub.execute_input":"2024-08-17T15:35:12.978231Z","iopub.status.idle":"2024-08-17T15:35:13.034348Z","shell.execute_reply.started":"2024-08-17T15:35:12.978190Z","shell.execute_reply":"2024-08-17T15:35:13.033313Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# MLP model for classification\nmodel = nn.Sequential(\n    nn.Linear(4, 16),  # Input layer (4 features to 16 neurons)\n    nn.ReLU(),\n    nn.Linear(16, 3)   # Output layer (16 neurons to 3 classes)\n)\n\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training the model\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for features, labels in train_loader:\n        features, labels = features.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(features)\n        \n        # Calculate loss\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T15:35:47.434386Z","iopub.execute_input":"2024-08-17T15:35:47.434762Z","iopub.status.idle":"2024-08-17T15:35:49.686356Z","shell.execute_reply.started":"2024-08-17T15:35:47.434732Z","shell.execute_reply":"2024-08-17T15:35:49.685110Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch [1/50], Loss: 1.0966\nEpoch [2/50], Loss: 1.0687\nEpoch [3/50], Loss: 1.0388\nEpoch [4/50], Loss: 1.0050\nEpoch [5/50], Loss: 0.9697\nEpoch [6/50], Loss: 0.9510\nEpoch [7/50], Loss: 0.9125\nEpoch [8/50], Loss: 0.8743\nEpoch [9/50], Loss: 0.8421\nEpoch [10/50], Loss: 0.8344\nEpoch [11/50], Loss: 0.7967\nEpoch [12/50], Loss: 0.7855\nEpoch [13/50], Loss: 0.7444\nEpoch [14/50], Loss: 0.7088\nEpoch [15/50], Loss: 0.6969\nEpoch [16/50], Loss: 0.6488\nEpoch [17/50], Loss: 0.6473\nEpoch [18/50], Loss: 0.6159\nEpoch [19/50], Loss: 0.5955\nEpoch [20/50], Loss: 0.5825\nEpoch [21/50], Loss: 0.5616\nEpoch [22/50], Loss: 0.5413\nEpoch [23/50], Loss: 0.5390\nEpoch [24/50], Loss: 0.5250\nEpoch [25/50], Loss: 0.5053\nEpoch [26/50], Loss: 0.5066\nEpoch [27/50], Loss: 0.4943\nEpoch [28/50], Loss: 0.4892\nEpoch [29/50], Loss: 0.4724\nEpoch [30/50], Loss: 0.4611\nEpoch [31/50], Loss: 0.4501\nEpoch [32/50], Loss: 0.4435\nEpoch [33/50], Loss: 0.4335\nEpoch [34/50], Loss: 0.4255\nEpoch [35/50], Loss: 0.4234\nEpoch [36/50], Loss: 0.4115\nEpoch [37/50], Loss: 0.4075\nEpoch [38/50], Loss: 0.3889\nEpoch [39/50], Loss: 0.3966\nEpoch [40/50], Loss: 0.3906\nEpoch [41/50], Loss: 0.3792\nEpoch [42/50], Loss: 0.3577\nEpoch [43/50], Loss: 0.3556\nEpoch [44/50], Loss: 0.3694\nEpoch [45/50], Loss: 0.3561\nEpoch [46/50], Loss: 0.3483\nEpoch [47/50], Loss: 0.3424\nEpoch [48/50], Loss: 0.3396\nEpoch [49/50], Loss: 0.3421\nEpoch [50/50], Loss: 0.3305\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for features, labels in test_loader:\n        features, labels = features.to(device), labels.to(device)\n        outputs = model(features)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the model on the test dataset: {100 * correct / total:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T15:36:03.911009Z","iopub.execute_input":"2024-08-17T15:36:03.911579Z","iopub.status.idle":"2024-08-17T15:36:03.926886Z","shell.execute_reply.started":"2024-08-17T15:36:03.911545Z","shell.execute_reply":"2024-08-17T15:36:03.925610Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Accuracy of the model on the test dataset: 93.33%\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'iris_classification_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-17T15:37:24.621931Z","iopub.execute_input":"2024-08-17T15:37:24.622738Z","iopub.status.idle":"2024-08-17T15:37:24.629067Z","shell.execute_reply.started":"2024-08-17T15:37:24.622697Z","shell.execute_reply":"2024-08-17T15:37:24.627816Z"},"trusted":true},"execution_count":6,"outputs":[]}]}